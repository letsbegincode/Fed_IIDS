{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### **Section 1: Data Ingestion and Initial Consolidation**\n",
        "This section handles the initial loading of the IoT-DIAD Dataset, which is spread across multiple CSV files. It mounts Google Drive, recursively finds all `.csv` files, and then consolidates them into a single, large HDF5 file for easier processing. During this process, column names are standardized to ensure consistency."
      ],
      "metadata": {
        "id": "sGZwKYSH5yok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import glob\n",
        "import os\n",
        "\n",
        "# 1. Mount Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# --- EDIT THIS ---\n",
        "# 2. Point this to the main folder on your Drive\n",
        "DRIVE_DATA_FOLDER = '/content/drive/MyDrive/FL_Project/IoT-DIAD-Dataset'\n",
        "# -----------------\n",
        "\n",
        "print(f\"Looking for CSVs in: {DRIVE_DATA_FOLDER}\")\n",
        "\n",
        "# 3. Find all .csv files recursively\n",
        "# This will search inside 'Benign', 'Brute Force', etc.\n",
        "all_csv_files = glob.glob(f\"{DRIVE_DATA_FOLDER}/**/*.csv\", recursive=True)\n",
        "\n",
        "if not all_csv_files:\n",
        "    print(\"ðŸš¨ ERROR: No CSV files found. Check your DRIVE_DATA_FOLDER path.\")\n",
        "else:\n",
        "    print(f\"Found {len(all_csv_files)} CSV files to process.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jMQuhenQASU",
        "outputId": "1639cf3f-0c54-4fde-d980-17f2731b8b32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Looking for CSVs in: /content/drive/MyDrive/FL_Project/IoT-DIAD-Dataset\n",
            "Found 23 CSV files to process.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Section 2: Data Cleaning and Preprocessing**\n",
        "After consolidating the raw data, this section focuses on cleaning and preparing the dataset for model training. It involves several critical steps:\n",
        "- **Dropping Unnecessary Columns**: Identifying and removing columns that are either identifiers (e.g., `flow_id`, `timestamp`, `src_ip`, `dst_ip`) or less relevant for a machine learning model.\n",
        "- **Handling Missing Values**: Removing rows that contain any null values to ensure data integrity, assuming a sufficient amount of data remains.\n",
        "- **Replacing Infinite Values**: Converting any infinity values (which often arise from division by zero) to zero to prevent errors in numerical computations.\n",
        "\n",
        "This stage produces a `master_cleaned_dataset.h5` file."
      ],
      "metadata": {
        "id": "RgMa7Q_q56O5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import os\n",
        "\n",
        "# --- EDIT THIS ---\n",
        "MASTER_HDF_FILE = '/content/drive/MyDrive/FL_Project/master_labeled_dataset.h5'\n",
        "# -----------------\n",
        "HDF_KEY = 'data'\n",
        "\n",
        "def sanitize_column_name(col_name):\n",
        "    \"\"\"Cleans column names to be HDF5-safe.\"\"\"\n",
        "    col_name = col_name.strip()\n",
        "    col_name = re.sub(r'[^a-zA-Z0-9_]+', '_', col_name)\n",
        "    col_name = col_name.lower()\n",
        "    return col_name\n",
        "\n",
        "# Clean up any old file first\n",
        "if os.path.exists(MASTER_HDF_FILE):\n",
        "    os.remove(MASTER_HDF_FILE)\n",
        "    print(f\"Removed old version of {MASTER_HDF_FILE}\")\n",
        "\n",
        "print(\"Starting FAST 'all-at-once' aggregation and sanitization...\")\n",
        "\n",
        "# We will store all DataFrames in this list\n",
        "all_dataframes = []\n",
        "master_columns = None\n",
        "total_rows = 0\n",
        "\n",
        "for i, filepath in enumerate(all_csv_files):\n",
        "    try:\n",
        "        print(f\"  Processing file {i+1}/{len(all_csv_files)}: {filepath}\")\n",
        "\n",
        "        # Read the ENTIRE file at once. No chunking.\n",
        "        # low_memory=False can speed up reading of mixed-type columns.\n",
        "        df_temp = pd.read_csv(filepath, low_memory=False)\n",
        "\n",
        "        if df_temp.empty:\n",
        "             print(f\"  Skipping empty file: {filepath}\")\n",
        "             continue\n",
        "\n",
        "        # --- Sanitize Column Names ---\n",
        "        clean_cols = [sanitize_column_name(col) for col in df_temp.columns]\n",
        "        df_temp.columns = clean_cols\n",
        "\n",
        "        # On the very first file, store the clean column names\n",
        "        if master_columns is None:\n",
        "            master_columns = df_temp.columns\n",
        "\n",
        "        # --- Schema Mismatch Check ---\n",
        "        # Ensure all other files match the first one\n",
        "        if list(df_temp.columns) != list(master_columns):\n",
        "            print(f\"  WARNING: Column mismatch in {filepath}. Re-aligning columns.\")\n",
        "            # This will add missing columns (with NaN) and drop extra ones\n",
        "            df_temp = df_temp.reindex(columns=master_columns)\n",
        "\n",
        "        all_dataframes.append(df_temp)\n",
        "        total_rows += len(df_temp)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"  ðŸš¨ ERROR processing {filepath}: {e}. Skipping file.\")\n",
        "\n",
        "print(\"\\n----------------------------------\")\n",
        "print(\"All files loaded into memory. Now concatenating...\")\n",
        "\n",
        "# --- This is the single \"big memory\" step ---\n",
        "# It combines all the dataframes in the list into one.\n",
        "df_master = pd.concat(all_dataframes, ignore_index=True)\n",
        "\n",
        "print(f\"Concatenation complete. Total rows: {total_rows}\")\n",
        "print(f\"Final shape: {df_master.shape}\")\n",
        "\n",
        "print(f\"Saving to HDF5 file: {MASTER_HDF_FILE}...\")\n",
        "# Save the single, final dataframe\n",
        "df_master.to_hdf(\n",
        "    MASTER_HDF_FILE,\n",
        "    HDF_KEY,\n",
        "    format='table',\n",
        "    mode='w' # 'w' for write (not append)\n",
        ")\n",
        "\n",
        "print(\"\\nâœ… All files merged and SANITIZED into one HDF5 file!\")\n",
        "print(f\"Master file saved at: {MASTER_HDF_FILE}\")\n",
        "print(\"----------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "NLoFeS-2QQsy",
        "outputId": "b7bf349b-884c-4a33-e1e7-6b002092146d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removed old version of /content/drive/MyDrive/FL_Project/master_labeled_dataset.h5\n",
            "Starting FAST 'all-at-once' aggregation and sanitization...\n",
            "  Processing file 1/23: /content/drive/MyDrive/FL_Project/IoT-DIAD-Dataset/Spoofing/DNS_Spoofing.pcap_Flow.csv\n",
            "  Processing file 2/23: /content/drive/MyDrive/FL_Project/IoT-DIAD-Dataset/Spoofing/MITM-ArpSpoofing.pcap_Flow.csv\n",
            "  Processing file 3/23: /content/drive/MyDrive/FL_Project/IoT-DIAD-Dataset/Recon/VulnerabilityScan.pcap_Flow.csv\n",
            "  Processing file 4/23: /content/drive/MyDrive/FL_Project/IoT-DIAD-Dataset/Web-Based/SqlInjection.pcap_Flow.csv\n",
            "  Processing file 5/23: /content/drive/MyDrive/FL_Project/IoT-DIAD-Dataset/Web-Based/Uploading_Attack.pcap_Flow.csv\n",
            "  Processing file 6/23: /content/drive/MyDrive/FL_Project/IoT-DIAD-Dataset/Web-Based/XSS.pcap_Flow.csv\n",
            "  Processing file 7/23: /content/drive/MyDrive/FL_Project/IoT-DIAD-Dataset/Mirai/Mirai-greeth_flood1.pcap_Flow.csv\n",
            "  Processing file 8/23: /content/drive/MyDrive/FL_Project/IoT-DIAD-Dataset/Mirai/Mirai-greeth_flood4.pcap_Flow.csv\n",
            "  Processing file 9/23: /content/drive/MyDrive/FL_Project/IoT-DIAD-Dataset/Mirai/Mirai-greeth_flood6.pcap_Flow.csv\n",
            "  Processing file 10/23: /content/drive/MyDrive/FL_Project/IoT-DIAD-Dataset/Mirai/Mirai-greeth_flood9.pcap_Flow.csv\n",
            "  Processing file 11/23: /content/drive/MyDrive/FL_Project/IoT-DIAD-Dataset/Mirai/Mirai-greeth_flood13.pcap_Flow.csv\n",
            "  Processing file 12/23: /content/drive/MyDrive/FL_Project/IoT-DIAD-Dataset/Mirai/Mirai-greeth_flood28.pcap_Flow.csv\n",
            "  Processing file 13/23: /content/drive/MyDrive/FL_Project/IoT-DIAD-Dataset/DOS/DoS-UDP_Flood8.pcap_Flow.csv\n",
            "  Processing file 14/23: /content/drive/MyDrive/FL_Project/IoT-DIAD-Dataset/DOS/DoS-UDP_Flood9.pcap_Flow.csv\n",
            "  Processing file 15/23: /content/drive/MyDrive/FL_Project/IoT-DIAD-Dataset/DDOS/DDoS-ACK_Fragmentation8.pcap_Flow.csv\n",
            "  Processing file 16/23: /content/drive/MyDrive/FL_Project/IoT-DIAD-Dataset/DDOS/DDoS-ACK_Fragmentation11.pcap_Flow.csv\n",
            "  Processing file 17/23: /content/drive/MyDrive/FL_Project/IoT-DIAD-Dataset/DDOS/DDoS-ICMP_Flood4.pcap_Flow.csv\n",
            "  Processing file 18/23: /content/drive/MyDrive/FL_Project/IoT-DIAD-Dataset/DDOS/DDoS-ICMP_Fragmentation10.pcap_Flow.csv\n",
            "  Processing file 19/23: /content/drive/MyDrive/FL_Project/IoT-DIAD-Dataset/Brute Force/DictionaryBruteForce.pcap_Flow.csv\n",
            "  Processing file 20/23: /content/drive/MyDrive/FL_Project/IoT-DIAD-Dataset/Benign/BenignTraffic.pcap_Flow.csv\n",
            "  Processing file 21/23: /content/drive/MyDrive/FL_Project/IoT-DIAD-Dataset/Benign/BenignTraffic1.pcap_Flow.csv\n",
            "  Processing file 22/23: /content/drive/MyDrive/FL_Project/IoT-DIAD-Dataset/Benign/BenignTraffic2.pcap_Flow.csv\n",
            "  Processing file 23/23: /content/drive/MyDrive/FL_Project/IoT-DIAD-Dataset/Benign/BenignTraffic3.pcap_Flow.csv\n",
            "\n",
            "----------------------------------\n",
            "All files loaded into memory. Now concatenating...\n",
            "Concatenation complete. Total rows: 1619747\n",
            "Final shape: (1619747, 84)\n",
            "Saving to HDF5 file: /content/drive/MyDrive/FL_Project/master_labeled_dataset.h5...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1534407336.py:74: FutureWarning: Starting with pandas version 3.0 all arguments of to_hdf except for the argument 'path_or_buf' will be keyword-only.\n",
            "  df_master.to_hdf(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "âœ… All files merged and SANITIZED into one HDF5 file!\n",
            "Master file saved at: /content/drive/MyDrive/FL_Project/master_labeled_dataset.h5\n",
            "----------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_master.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbOrs3_dRV7s",
        "outputId": "a600e827-e6de-4f44-8130-84444af72372"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1619747 entries, 0 to 1619746\n",
            "Data columns (total 84 columns):\n",
            " #   Column                      Non-Null Count    Dtype  \n",
            "---  ------                      --------------    -----  \n",
            " 0   flow_id                     1619747 non-null  object \n",
            " 1   src_ip                      1619747 non-null  object \n",
            " 2   src_port                    1619747 non-null  int64  \n",
            " 3   dst_ip                      1619747 non-null  object \n",
            " 4   dst_port                    1619747 non-null  int64  \n",
            " 5   protocol                    1619747 non-null  int64  \n",
            " 6   timestamp                   1619747 non-null  object \n",
            " 7   flow_duration               1619747 non-null  int64  \n",
            " 8   total_fwd_packet            1619747 non-null  int64  \n",
            " 9   total_bwd_packets           1619747 non-null  int64  \n",
            " 10  total_length_of_fwd_packet  1619747 non-null  float64\n",
            " 11  total_length_of_bwd_packet  1619747 non-null  float64\n",
            " 12  fwd_packet_length_max       1619747 non-null  float64\n",
            " 13  fwd_packet_length_min       1619747 non-null  float64\n",
            " 14  fwd_packet_length_mean      1619747 non-null  float64\n",
            " 15  fwd_packet_length_std       1619747 non-null  float64\n",
            " 16  bwd_packet_length_max       1619747 non-null  float64\n",
            " 17  bwd_packet_length_min       1619747 non-null  float64\n",
            " 18  bwd_packet_length_mean      1619747 non-null  float64\n",
            " 19  bwd_packet_length_std       1619747 non-null  float64\n",
            " 20  flow_bytes_s                1618373 non-null  float64\n",
            " 21  flow_packets_s              1619747 non-null  float64\n",
            " 22  flow_iat_mean               1619747 non-null  float64\n",
            " 23  flow_iat_std                1619747 non-null  float64\n",
            " 24  flow_iat_max                1619747 non-null  float64\n",
            " 25  flow_iat_min                1619747 non-null  float64\n",
            " 26  fwd_iat_total               1619747 non-null  float64\n",
            " 27  fwd_iat_mean                1619747 non-null  float64\n",
            " 28  fwd_iat_std                 1619747 non-null  float64\n",
            " 29  fwd_iat_max                 1619747 non-null  float64\n",
            " 30  fwd_iat_min                 1619747 non-null  float64\n",
            " 31  bwd_iat_total               1619747 non-null  float64\n",
            " 32  bwd_iat_mean                1619747 non-null  float64\n",
            " 33  bwd_iat_std                 1619747 non-null  float64\n",
            " 34  bwd_iat_max                 1619747 non-null  float64\n",
            " 35  bwd_iat_min                 1619747 non-null  float64\n",
            " 36  fwd_psh_flags               1619747 non-null  int64  \n",
            " 37  bwd_psh_flags               1619747 non-null  int64  \n",
            " 38  fwd_urg_flags               1619747 non-null  int64  \n",
            " 39  bwd_urg_flags               1619747 non-null  int64  \n",
            " 40  fwd_header_length           1619747 non-null  int64  \n",
            " 41  bwd_header_length           1619747 non-null  int64  \n",
            " 42  fwd_packets_s               1619747 non-null  float64\n",
            " 43  bwd_packets_s               1619747 non-null  float64\n",
            " 44  packet_length_min           1619747 non-null  float64\n",
            " 45  packet_length_max           1619747 non-null  float64\n",
            " 46  packet_length_mean          1619747 non-null  float64\n",
            " 47  packet_length_std           1619747 non-null  float64\n",
            " 48  packet_length_variance      1619747 non-null  float64\n",
            " 49  fin_flag_count              1619747 non-null  int64  \n",
            " 50  syn_flag_count              1619747 non-null  int64  \n",
            " 51  rst_flag_count              1619747 non-null  int64  \n",
            " 52  psh_flag_count              1619747 non-null  int64  \n",
            " 53  ack_flag_count              1619747 non-null  int64  \n",
            " 54  urg_flag_count              1619747 non-null  int64  \n",
            " 55  cwr_flag_count              1619747 non-null  int64  \n",
            " 56  ece_flag_count              1619747 non-null  int64  \n",
            " 57  down_up_ratio               1619747 non-null  float64\n",
            " 58  average_packet_size         1619747 non-null  float64\n",
            " 59  fwd_segment_size_avg        1619747 non-null  float64\n",
            " 60  bwd_segment_size_avg        1619747 non-null  float64\n",
            " 61  fwd_bytes_bulk_avg          1619747 non-null  int64  \n",
            " 62  fwd_packet_bulk_avg         1619747 non-null  int64  \n",
            " 63  fwd_bulk_rate_avg           1619747 non-null  int64  \n",
            " 64  bwd_bytes_bulk_avg          1619747 non-null  int64  \n",
            " 65  bwd_packet_bulk_avg         1619747 non-null  int64  \n",
            " 66  bwd_bulk_rate_avg           1619747 non-null  int64  \n",
            " 67  subflow_fwd_packets         1619747 non-null  int64  \n",
            " 68  subflow_fwd_bytes           1619747 non-null  int64  \n",
            " 69  subflow_bwd_packets         1619747 non-null  int64  \n",
            " 70  subflow_bwd_bytes           1619747 non-null  int64  \n",
            " 71  fwd_init_win_bytes          1619747 non-null  int64  \n",
            " 72  bwd_init_win_bytes          1619747 non-null  int64  \n",
            " 73  fwd_act_data_pkts           1619747 non-null  int64  \n",
            " 74  fwd_seg_size_min            1619747 non-null  int64  \n",
            " 75  active_mean                 1619747 non-null  float64\n",
            " 76  active_std                  1619747 non-null  float64\n",
            " 77  active_max                  1619747 non-null  float64\n",
            " 78  active_min                  1619747 non-null  float64\n",
            " 79  idle_mean                   1619747 non-null  float64\n",
            " 80  idle_std                    1619747 non-null  float64\n",
            " 81  idle_max                    1619747 non-null  float64\n",
            " 82  idle_min                    1619747 non-null  float64\n",
            " 83  label                       1619747 non-null  object \n",
            "dtypes: float64(45), int64(34), object(5)\n",
            "memory usage: 1.0+ GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Section 3: Feature Engineering and Scaling**\n",
        "This section prepares the cleaned data for machine learning by transforming raw features into a suitable format. It involves:\n",
        "- **Identifying Feature Types**: Distinguishing between numeric and categorical features.\n",
        "- **Scaling Numeric Features**: Applying `MinMaxScaler` to numeric features to normalize their range, which is crucial for many machine learning algorithms.\n",
        "- **Hashing Categorical Features**: Using `FeatureHasher` to convert high-cardinality categorical features into a fixed-size numerical vector, reducing dimensionality while retaining information.\n",
        "- **Saving Preprocessors**: Storing the fitted `MinMaxScaler` and `FeatureHasher` to ensure consistency when processing new, unseen data.\n",
        "\n",
        "The output is a `final_processed_data.npz` file containing the scaled feature matrix and original string labels."
      ],
      "metadata": {
        "id": "qqehwLI96IK9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "# 1. The master file you just created\n",
        "MASTER_HDF_FILE = '/content/drive/MyDrive/FL_Project/master_labeled_dataset.h5'\n",
        "HDF_KEY = 'data'\n",
        "\n",
        "# 2. The *new* clean file we will create\n",
        "CLEANED_HDF_FILE = '/content/drive/MyDrive/FL_Project/master_cleaned_dataset.h5'\n",
        "\n",
        "# 3. Columns to drop\n",
        "# These are identifiers or high-cardinality features that are\n",
        "# not good for a simple DNN. We also drop the string labels\n",
        "# we created manually.\n",
        "UNNECESSARY_COLS = [\n",
        "    'flow_id',\n",
        "    'timestamp',\n",
        "    'src_ip',\n",
        "    'dst_ip',\n",
        "]\n",
        "# ---------------------\n",
        "\n",
        "print(f\"Loading master file: {MASTER_HDF_FILE}\")\n",
        "# 1. Load the entire 1GB file into memory\n",
        "df_master = pd.read_hdf(MASTER_HDF_FILE, HDF_KEY)\n",
        "print(\"Load complete.\")\n",
        "\n",
        "print(\"\\n--- BEFORE CLEANING ---\")\n",
        "print(df_master.info())\n",
        "\n",
        "# 2. Drop unnecessary columns\n",
        "# We will keep 'label' for now to do our non-IID split later\n",
        "cols_to_drop = [col for col in UNNECESSARY_COLS if col in df_master.columns]\n",
        "df_master.drop(columns=cols_to_drop, inplace=True)\n",
        "print(f\"\\nDropped {len(cols_to_drop)} unnecessary columns.\")\n",
        "\n",
        "# 3. Drop all rows with ANY null/missing values\n",
        "# As you said, we have enough data to do this.\n",
        "initial_rows = len(df_master)\n",
        "df_master.dropna(inplace=True)\n",
        "final_rows = len(df_master)\n",
        "print(f\"Dropped {initial_rows - final_rows} rows with null values.\")\n",
        "\n",
        "# 4. Replace any lingering infinity values (from division by zero)\n",
        "df_master.replace([np.inf, -np.inf], 0, inplace=True)\n",
        "print(\"Replaced all -inf/+inf values with 0.\")\n",
        "\n",
        "print(\"\\n--- AFTER CLEANING ---\")\n",
        "print(f\"Final shape: {df_master.shape}\")\n",
        "print(df_master.info())\n",
        "\n",
        "# 5. Save the new, clean DataFrame\n",
        "print(f\"\\nSaving clean data to: {CLEANED_HDF_FILE}\")\n",
        "df_master.to_hdf(\n",
        "    CLEANED_HDF_FILE,\n",
        "    HDF_KEY,\n",
        "    format='table',\n",
        "    mode='w'\n",
        ")\n",
        "\n",
        "print(\"âœ… Clean file saved to Google Drive.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BDqXh6rxUJwK",
        "outputId": "0702e403-c426-4831-fb2b-d298a7f5563d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading master file: /content/drive/MyDrive/FL_Project/master_labeled_dataset.h5\n",
            "Load complete.\n",
            "\n",
            "--- BEFORE CLEANING ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 1619747 entries, 0 to 1619746\n",
            "Data columns (total 84 columns):\n",
            " #   Column                      Non-Null Count    Dtype  \n",
            "---  ------                      --------------    -----  \n",
            " 0   flow_id                     1619747 non-null  object \n",
            " 1   src_ip                      1619747 non-null  object \n",
            " 2   src_port                    1619747 non-null  int64  \n",
            " 3   dst_ip                      1619747 non-null  object \n",
            " 4   dst_port                    1619747 non-null  int64  \n",
            " 5   protocol                    1619747 non-null  int64  \n",
            " 6   timestamp                   1619747 non-null  object \n",
            " 7   flow_duration               1619747 non-null  int64  \n",
            " 8   total_fwd_packet            1619747 non-null  int64  \n",
            " 9   total_bwd_packets           1619747 non-null  int64  \n",
            " 10  total_length_of_fwd_packet  1619747 non-null  float64\n",
            " 11  total_length_of_bwd_packet  1619747 non-null  float64\n",
            " 12  fwd_packet_length_max       1619747 non-null  float64\n",
            " 13  fwd_packet_length_min       1619747 non-null  float64\n",
            " 14  fwd_packet_length_mean      1619747 non-null  float64\n",
            " 15  fwd_packet_length_std       1619747 non-null  float64\n",
            " 16  bwd_packet_length_max       1619747 non-null  float64\n",
            " 17  bwd_packet_length_min       1619747 non-null  float64\n",
            " 18  bwd_packet_length_mean      1619747 non-null  float64\n",
            " 19  bwd_packet_length_std       1619747 non-null  float64\n",
            " 20  flow_bytes_s                1618373 non-null  float64\n",
            " 21  flow_packets_s              1619747 non-null  float64\n",
            " 22  flow_iat_mean               1619747 non-null  float64\n",
            " 23  flow_iat_std                1619747 non-null  float64\n",
            " 24  flow_iat_max                1619747 non-null  float64\n",
            " 25  flow_iat_min                1619747 non-null  float64\n",
            " 26  fwd_iat_total               1619747 non-null  float64\n",
            " 27  fwd_iat_mean                1619747 non-null  float64\n",
            " 28  fwd_iat_std                 1619747 non-null  float64\n",
            " 29  fwd_iat_max                 1619747 non-null  float64\n",
            " 30  fwd_iat_min                 1619747 non-null  float64\n",
            " 31  bwd_iat_total               1619747 non-null  float64\n",
            " 32  bwd_iat_mean                1619747 non-null  float64\n",
            " 33  bwd_iat_std                 1619747 non-null  float64\n",
            " 34  bwd_iat_max                 1619747 non-null  float64\n",
            " 35  bwd_iat_min                 1619747 non-null  float64\n",
            " 36  fwd_psh_flags               1619747 non-null  int64  \n",
            " 37  bwd_psh_flags               1619747 non-null  int64  \n",
            " 38  fwd_urg_flags               1619747 non-null  int64  \n",
            " 39  bwd_urg_flags               1619747 non-null  int64  \n",
            " 40  fwd_header_length           1619747 non-null  int64  \n",
            " 41  bwd_header_length           1619747 non-null  int64  \n",
            " 42  fwd_packets_s               1619747 non-null  float64\n",
            " 43  bwd_packets_s               1619747 non-null  float64\n",
            " 44  packet_length_min           1619747 non-null  float64\n",
            " 45  packet_length_max           1619747 non-null  float64\n",
            " 46  packet_length_mean          1619747 non-null  float64\n",
            " 47  packet_length_std           1619747 non-null  float64\n",
            " 48  packet_length_variance      1619747 non-null  float64\n",
            " 49  fin_flag_count              1619747 non-null  int64  \n",
            " 50  syn_flag_count              1619747 non-null  int64  \n",
            " 51  rst_flag_count              1619747 non-null  int64  \n",
            " 52  psh_flag_count              1619747 non-null  int64  \n",
            " 53  ack_flag_count              1619747 non-null  int64  \n",
            " 54  urg_flag_count              1619747 non-null  int64  \n",
            " 55  cwr_flag_count              1619747 non-null  int64  \n",
            " 56  ece_flag_count              1619747 non-null  int64  \n",
            " 57  down_up_ratio               1619747 non-null  float64\n",
            " 58  average_packet_size         1619747 non-null  float64\n",
            " 59  fwd_segment_size_avg        1619747 non-null  float64\n",
            " 60  bwd_segment_size_avg        1619747 non-null  float64\n",
            " 61  fwd_bytes_bulk_avg          1619747 non-null  int64  \n",
            " 62  fwd_packet_bulk_avg         1619747 non-null  int64  \n",
            " 63  fwd_bulk_rate_avg           1619747 non-null  int64  \n",
            " 64  bwd_bytes_bulk_avg          1619747 non-null  int64  \n",
            " 65  bwd_packet_bulk_avg         1619747 non-null  int64  \n",
            " 66  bwd_bulk_rate_avg           1619747 non-null  int64  \n",
            " 67  subflow_fwd_packets         1619747 non-null  int64  \n",
            " 68  subflow_fwd_bytes           1619747 non-null  int64  \n",
            " 69  subflow_bwd_packets         1619747 non-null  int64  \n",
            " 70  subflow_bwd_bytes           1619747 non-null  int64  \n",
            " 71  fwd_init_win_bytes          1619747 non-null  int64  \n",
            " 72  bwd_init_win_bytes          1619747 non-null  int64  \n",
            " 73  fwd_act_data_pkts           1619747 non-null  int64  \n",
            " 74  fwd_seg_size_min            1619747 non-null  int64  \n",
            " 75  active_mean                 1619747 non-null  float64\n",
            " 76  active_std                  1619747 non-null  float64\n",
            " 77  active_max                  1619747 non-null  float64\n",
            " 78  active_min                  1619747 non-null  float64\n",
            " 79  idle_mean                   1619747 non-null  float64\n",
            " 80  idle_std                    1619747 non-null  float64\n",
            " 81  idle_max                    1619747 non-null  float64\n",
            " 82  idle_min                    1619747 non-null  float64\n",
            " 83  label                       1619747 non-null  object \n",
            "dtypes: float64(45), int64(34), object(5)\n",
            "memory usage: 1.0+ GB\n",
            "None\n",
            "\n",
            "Dropped 4 unnecessary columns.\n",
            "Dropped 1374 rows with null values.\n",
            "Replaced all -inf/+inf values with 0.\n",
            "\n",
            "--- AFTER CLEANING ---\n",
            "Final shape: (1618373, 80)\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 1618373 entries, 0 to 1619746\n",
            "Data columns (total 80 columns):\n",
            " #   Column                      Non-Null Count    Dtype  \n",
            "---  ------                      --------------    -----  \n",
            " 0   src_port                    1618373 non-null  int64  \n",
            " 1   dst_port                    1618373 non-null  int64  \n",
            " 2   protocol                    1618373 non-null  int64  \n",
            " 3   flow_duration               1618373 non-null  int64  \n",
            " 4   total_fwd_packet            1618373 non-null  int64  \n",
            " 5   total_bwd_packets           1618373 non-null  int64  \n",
            " 6   total_length_of_fwd_packet  1618373 non-null  float64\n",
            " 7   total_length_of_bwd_packet  1618373 non-null  float64\n",
            " 8   fwd_packet_length_max       1618373 non-null  float64\n",
            " 9   fwd_packet_length_min       1618373 non-null  float64\n",
            " 10  fwd_packet_length_mean      1618373 non-null  float64\n",
            " 11  fwd_packet_length_std       1618373 non-null  float64\n",
            " 12  bwd_packet_length_max       1618373 non-null  float64\n",
            " 13  bwd_packet_length_min       1618373 non-null  float64\n",
            " 14  bwd_packet_length_mean      1618373 non-null  float64\n",
            " 15  bwd_packet_length_std       1618373 non-null  float64\n",
            " 16  flow_bytes_s                1618373 non-null  float64\n",
            " 17  flow_packets_s              1618373 non-null  float64\n",
            " 18  flow_iat_mean               1618373 non-null  float64\n",
            " 19  flow_iat_std                1618373 non-null  float64\n",
            " 20  flow_iat_max                1618373 non-null  float64\n",
            " 21  flow_iat_min                1618373 non-null  float64\n",
            " 22  fwd_iat_total               1618373 non-null  float64\n",
            " 23  fwd_iat_mean                1618373 non-null  float64\n",
            " 24  fwd_iat_std                 1618373 non-null  float64\n",
            " 25  fwd_iat_max                 1618373 non-null  float64\n",
            " 26  fwd_iat_min                 1618373 non-null  float64\n",
            " 27  bwd_iat_total               1618373 non-null  float64\n",
            " 28  bwd_iat_mean                1618373 non-null  float64\n",
            " 29  bwd_iat_std                 1618373 non-null  float64\n",
            " 30  bwd_iat_max                 1618373 non-null  float64\n",
            " 31  bwd_iat_min                 1618373 non-null  float64\n",
            " 32  fwd_psh_flags               1618373 non-null  int64  \n",
            " 33  bwd_psh_flags               1618373 non-null  int64  \n",
            " 34  fwd_urg_flags               1618373 non-null  int64  \n",
            " 35  bwd_urg_flags               1618373 non-null  int64  \n",
            " 36  fwd_header_length           1618373 non-null  int64  \n",
            " 37  bwd_header_length           1618373 non-null  int64  \n",
            " 38  fwd_packets_s               1618373 non-null  float64\n",
            " 39  bwd_packets_s               1618373 non-null  float64\n",
            " 40  packet_length_min           1618373 non-null  float64\n",
            " 41  packet_length_max           1618373 non-null  float64\n",
            " 42  packet_length_mean          1618373 non-null  float64\n",
            " 43  packet_length_std           1618373 non-null  float64\n",
            " 44  packet_length_variance      1618373 non-null  float64\n",
            " 45  fin_flag_count              1618373 non-null  int64  \n",
            " 46  syn_flag_count              1618373 non-null  int64  \n",
            " 47  rst_flag_count              1618373 non-null  int64  \n",
            " 48  psh_flag_count              1618373 non-null  int64  \n",
            " 49  ack_flag_count              1618373 non-null  int64  \n",
            " 50  urg_flag_count              1618373 non-null  int64  \n",
            " 51  cwr_flag_count              1618373 non-null  int64  \n",
            " 52  ece_flag_count              1618373 non-null  int64  \n",
            " 53  down_up_ratio               1618373 non-null  float64\n",
            " 54  average_packet_size         1618373 non-null  float64\n",
            " 55  fwd_segment_size_avg        1618373 non-null  float64\n",
            " 56  bwd_segment_size_avg        1618373 non-null  float64\n",
            " 57  fwd_bytes_bulk_avg          1618373 non-null  int64  \n",
            " 58  fwd_packet_bulk_avg         1618373 non-null  int64  \n",
            " 59  fwd_bulk_rate_avg           1618373 non-null  int64  \n",
            " 60  bwd_bytes_bulk_avg          1618373 non-null  int64  \n",
            " 61  bwd_packet_bulk_avg         1618373 non-null  int64  \n",
            " 62  bwd_bulk_rate_avg           1618373 non-null  int64  \n",
            " 63  subflow_fwd_packets         1618373 non-null  int64  \n",
            " 64  subflow_fwd_bytes           1618373 non-null  int64  \n",
            " 65  subflow_bwd_packets         1618373 non-null  int64  \n",
            " 66  subflow_bwd_bytes           1618373 non-null  int64  \n",
            " 67  fwd_init_win_bytes          1618373 non-null  int64  \n",
            " 68  bwd_init_win_bytes          1618373 non-null  int64  \n",
            " 69  fwd_act_data_pkts           1618373 non-null  int64  \n",
            " 70  fwd_seg_size_min            1618373 non-null  int64  \n",
            " 71  active_mean                 1618373 non-null  float64\n",
            " 72  active_std                  1618373 non-null  float64\n",
            " 73  active_max                  1618373 non-null  float64\n",
            " 74  active_min                  1618373 non-null  float64\n",
            " 75  idle_mean                   1618373 non-null  float64\n",
            " 76  idle_std                    1618373 non-null  float64\n",
            " 77  idle_max                    1618373 non-null  float64\n",
            " 78  idle_min                    1618373 non-null  float64\n",
            " 79  label                       1618373 non-null  object \n",
            "dtypes: float64(45), int64(34), object(1)\n",
            "memory usage: 1000.1+ MB\n",
            "None\n",
            "\n",
            "Saving clean data to: /content/drive/MyDrive/FL_Project/master_cleaned_dataset.h5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-8720150.py:56: FutureWarning: Starting with pandas version 3.0 all arguments of to_hdf except for the argument 'path_or_buf' will be keyword-only.\n",
            "  df_master.to_hdf(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Clean file saved to Google Drive.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Section 4: Feature Reduction**\n",
        "To optimize model performance, reduce training time, and mitigate the curse of dimensionality, this section employs two feature reduction techniques:\n",
        "- **Correlation Filtering**: Eliminating highly correlated features to avoid multicollinearity and redundancy.\n",
        "- **Tree-Based Feature Selection (LightGBM)**: Utilizing a LightGBM classifier to rank features by importance and select the top `K` most relevant features for the given task.\n",
        "\n",
        "The chosen features form a 'pipeline' that is saved, along with the `final_reduced_data.npz` file, ready for the next stage."
      ],
      "metadata": {
        "id": "QLcHUBDm6PLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.feature_extraction import FeatureHasher\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# --- Configuration ---\n",
        "# 1. The clean file from Cell 3\n",
        "CLEANED_HDF_FILE = '/content/drive/MyDrive/FL_Project/master_cleaned_dataset.h5'\n",
        "HDF_KEY = 'data'\n",
        "\n",
        "# 2. The path to save your *fitted* preprocessors\n",
        "PREPROCESSOR_PATH = '/content/drive/MyDrive/FL_Project/preprocessors.pkl'\n",
        "\n",
        "# 3. The path to save your *final, processed* data\n",
        "FINAL_PROCESSED_FILE = '/content/drive/MyDrive/FL_Project/final_processed_data.npz'\n",
        "# ---------------------\n",
        "\n",
        "print(f\"Loading clean data from: {CLEANED_HDF_FILE}\")\n",
        "# 1. Load the clean dataset\n",
        "df_clean = pd.read_hdf(CLEANED_HDF_FILE, HDF_KEY)\n",
        "print(\"Load complete.\")\n",
        "\n",
        "# --- 2. Identify Feature Columns Programmatically ---\n",
        "\n",
        "# a. Define the label column\n",
        "LABEL_COL = 'label'\n",
        "\n",
        "# b. Define known categorical features (even if they are 'int' type)\n",
        "#    These are features that represent categories, not quantities.\n",
        "KNOWN_CATEGORICAL = [\n",
        "    'src_port',\n",
        "    'dst_port',\n",
        "    'protocol',\n",
        "    'fwd_psh_flags',\n",
        "    'bwd_psh_flags',\n",
        "    'fwd_urg_flags',\n",
        "    'bwd_urg_flags',\n",
        "    'fin_flag_count',\n",
        "    'syn_flag_count',\n",
        "    'rst_flag_count',\n",
        "    'psh_flag_count',\n",
        "    'ack_flag_count',\n",
        "    'urg_flag_count',\n",
        "    'cwr_flag_count',\n",
        "    'ece_flag_count'\n",
        "]\n",
        "\n",
        "# Find which of these known categoricals actually exist in our clean DataFrame\n",
        "CATEGORICAL_COLS = [col for col in KNOWN_CATEGORICAL if col in df_clean.columns]\n",
        "\n",
        "# c. Numeric columns are everything else that is a number\n",
        "all_numeric_cols = df_clean.select_dtypes(include=[np.number]).columns\n",
        "NUMERIC_COLS = list(set(all_numeric_cols) - set(CATEGORICAL_COLS))\n",
        "\n",
        "# d. This is the string label we will use for splitting\n",
        "Y_LABELS_STRING = df_clean[LABEL_COL]\n",
        "\n",
        "print(f\"\\nIdentified {len(NUMERIC_COLS)} numeric features.\")\n",
        "print(f\"Identified {len(CATEGORICAL_COLS)} categorical features.\")\n",
        "print(f\"Identified {LABEL_COL} as the label.\")\n",
        "\n",
        "# --- 3. Fit and Save Preprocessors ---\n",
        "print(\"\\nFitting preprocessors on clean data...\")\n",
        "\n",
        "# a. Fit Scaler for numeric data\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(df_clean[NUMERIC_COLS])\n",
        "print(\"  MinMaxScaler fitted.\")\n",
        "\n",
        "# b. Initialize Hasher for categorical data\n",
        "N_FEATURES_HASH = 10  # We'll hash all categorical features down to 10\n",
        "hasher = FeatureHasher(n_features=N_FEATURES_HASH, input_type='string')\n",
        "print(f\"  FeatureHasher initialized with {N_FEATURES_HASH} features.\")\n",
        "\n",
        "# c. Save the fitted preprocessors to Google Drive\n",
        "with open(PREPROCESSOR_PATH, 'wb') as f:\n",
        "    pickle.dump({'scaler': scaler, 'hasher': hasher,\n",
        "                 'numeric_cols': NUMERIC_COLS,\n",
        "                 'categorical_cols': CATEGORICAL_COLS}, f)\n",
        "print(f\"Preprocessors saved to: {PREPROCESSOR_PATH}\")\n",
        "\n",
        "# --- 4. Transform the Entire Dataset ---\n",
        "print(\"\\nTransforming the full dataset in memory...\")\n",
        "\n",
        "# a. Scale all numeric data\n",
        "scaled_numeric_data = scaler.transform(df_clean[NUMERIC_COLS])\n",
        "print(f\"  Numeric data scaled. Shape: {scaled_numeric_data.shape}\")\n",
        "\n",
        "# b. Hash all categorical data\n",
        "#    Convert all to string, then to a dict for the hasher\n",
        "hashed_categorical_data = hasher.transform(df_clean[CATEGORICAL_COLS].astype(str).to_dict('records'))\n",
        "print(f\"  Categorical data hashed. Shape: {hashed_categorical_data.shape}\")\n",
        "\n",
        "# c. Combine into your final feature matrix 'X_processed'\n",
        "X_processed = np.hstack((scaled_numeric_data, hashed_categorical_data.toarray()))\n",
        "\n",
        "print(f\"\\nFinal feature matrix 'X_processed' created.\")\n",
        "print(f\"  Final X shape: {X_processed.shape}\")\n",
        "print(f\"  Final y shape: {Y_LABELS_STRING.shape}\")\n",
        "\n",
        "# --- 5. Save Final Processed Data ---\n",
        "# This saves our work before the final split, a very good checkpoint.\n",
        "print(f\"\\nSaving final processed data to: {FINAL_PROCESSED_FILE}\")\n",
        "np.savez_compressed(\n",
        "    FINAL_PROCESSED_FILE,\n",
        "    X=X_processed,\n",
        "    y_str=Y_LABELS_STRING.values  # Save the string labels for splitting\n",
        ")\n",
        "\n",
        "print(\"\\n----------------------------------\")\n",
        "print(\"âœ… Cell 4 Complete!\")\n",
        "print(\"Your data is now fully preprocessed and saved.\")\n",
        "print(\"You are ready for the final step: Cell 5 (Splitting).\")\n",
        "print(\"----------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y24nNlT3U2Oi",
        "outputId": "897ed955-64d8-4f78-c249-de0e70a36a13"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading clean data from: /content/drive/MyDrive/FL_Project/master_cleaned_dataset.h5\n",
            "Load complete.\n",
            "\n",
            "Identified 64 numeric features.\n",
            "Identified 15 categorical features.\n",
            "Identified label as the label.\n",
            "\n",
            "Fitting preprocessors on clean data...\n",
            "  MinMaxScaler fitted.\n",
            "  FeatureHasher initialized with 10 features.\n",
            "Preprocessors saved to: /content/drive/MyDrive/FL_Project/preprocessors.pkl\n",
            "\n",
            "Transforming the full dataset in memory...\n",
            "  Numeric data scaled. Shape: (1618373, 64)\n",
            "  Categorical data hashed. Shape: (1618373, 10)\n",
            "\n",
            "Final feature matrix 'X_processed' created.\n",
            "  Final X shape: (1618373, 74)\n",
            "  Final y shape: (1618373,)\n",
            "\n",
            "Saving final processed data to: /content/drive/MyDrive/FL_Project/final_processed_data.npz\n",
            "\n",
            "----------------------------------\n",
            "âœ… Cell 4 Complete!\n",
            "Your data is now fully preprocessed and saved.\n",
            "You are ready for the final step: Cell 5 (Splitting).\n",
            "----------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# First, we need to install LightGBM, which is a very fast\n",
        "# and powerful tree-based model for feature selection.\n",
        "!pip install lightgbm\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "import lightgbm as lgbm\n",
        "\n",
        "# --- Configuration ---\n",
        "# 1. The full, processed data file from Cell 4\n",
        "FINAL_PROCESSED_FILE = '/content/drive/MyDrive/FL_Project/final_processed_data.npz'\n",
        "\n",
        "# 2. The preprocessor file (to get our feature names)\n",
        "PREPROCESSOR_PATH = '/content/drive/MyDrive/FL_Project/preprocessors.pkl'\n",
        "\n",
        "# 3. The path to save our \"pipeline\" (the list of selected features)\n",
        "SELECTED_FEATURES_FILE = '/content/drive/MyDrive/FL_Project/selected_features.pkl'\n",
        "\n",
        "# 4. The final .npz file with *reduced* features\n",
        "FINAL_REDUCED_DATA_FILE = '/content/drive/MyDrive/FL_Project/final_reduced_data.npz'\n",
        "\n",
        "# 5. Reduction Parameters\n",
        "CORR_THRESHOLD = 0.95  # Drop features with > 95% correlation\n",
        "TOP_K_FEATURES = 30    # Select the 30 best features from the model\n",
        "# ---------------------\n",
        "\n",
        "print(f\"Loading full processed data from: {FINAL_PROCESSED_FILE}\")\n",
        "# 1. Load data from Cell 4\n",
        "# FIX: Add allow_pickle=True to load object arrays (y_str)\n",
        "data = np.load(FINAL_PROCESSED_FILE, allow_pickle=True)\n",
        "X_processed = data['X']\n",
        "y_labels_string = data['y_str']\n",
        "\n",
        "print(f\"Loading preprocessor info from: {PREPROCESSOR_PATH}\")\n",
        "# 2. Load preprocessor to get feature names\n",
        "with open(PREPROCESSOR_PATH, 'rb') as f:\n",
        "    preprocessors = pickle.load(f)\n",
        "\n",
        "NUMERIC_COLS = preprocessors['numeric_cols']\n",
        "CATEGORICAL_COLS = preprocessors['categorical_cols']\n",
        "N_FEATURES_HASH = 10 # This must match N_FEATURES_HASH from Cell 4\n",
        "\n",
        "# 3. Reconstruct the Full Feature Name List\n",
        "# This is critical for interpreting our results.\n",
        "hash_feature_names = [f'hash_{i}' for i in range(N_FEATURES_HASH)]\n",
        "all_feature_names = NUMERIC_COLS + hash_feature_names\n",
        "\n",
        "print(f\"Original feature count: {len(all_feature_names)}\")\n",
        "\n",
        "# 4. Create DataFrame for analysis\n",
        "# This is necessary for correlation and easy filtering.\n",
        "df_processed = pd.DataFrame(X_processed, columns=all_feature_names)\n",
        "\n",
        "# --- 5. Tech 1: Correlation Filtering ---\n",
        "print(f\"\\nRunning Tech 1: Correlation Filtering (Threshold={CORR_THRESHOLD})...\")\n",
        "corr_matrix = df_processed.corr().abs()\n",
        "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "cols_to_drop_corr = [col for col in upper_tri.columns if any(upper_tri[col] > CORR_THRESHOLD)]\n",
        "\n",
        "print(f\"  Found {len(cols_to_drop_corr)} highly correlated features to drop.\")\n",
        "print(f\"  Dropped features: {cols_to_drop_corr}\")\n",
        "\n",
        "# Drop these columns to create our first filtered dataset\n",
        "df_filtered_1 = df_processed.drop(columns=cols_to_drop_corr)\n",
        "print(f\"  Features remaining after correlation filtering: {len(df_filtered_1.columns)}\")\n",
        "\n",
        "# --- 6. Tech 2: Tree-Based Selection ---\n",
        "print(f\"\\nRunning Tech 2: Tree-Based Selection (Top K={TOP_K_FEATURES})...\")\n",
        "\n",
        "# a. Convert string labels to numbers (e.g., 'Benign'=0, 'DOS'=1, etc.)\n",
        "# This is needed to train the classifier.\n",
        "y_numeric, _ = pd.factorize(y_labels_string)\n",
        "\n",
        "# b. Initialize and train a LightGBM classifier\n",
        "# LGBM is very fast and memory-efficient.\n",
        "lgbm_selector = lgbm.LGBMClassifier(n_estimators=100, random_state=42, n_jobs=-1)\n",
        "\n",
        "print(\"  Training feature selection model...\")\n",
        "lgbm_selector.fit(df_filtered_1, y_numeric)\n",
        "print(\"  Model training complete.\")\n",
        "\n",
        "# c. Get feature importances\n",
        "importances = lgbm_selector.feature_importances_\n",
        "importance_df = pd.DataFrame({\n",
        "    'feature': df_filtered_1.columns,\n",
        "    'importance': importances\n",
        "}).sort_values(by='importance', ascending=False)\n",
        "\n",
        "# d. Select the Top K features\n",
        "final_selected_features = list(importance_df.head(TOP_K_FEATURES)['feature'])\n",
        "\n",
        "print(f\"\\n--- Top {TOP_K_FEATURES} Selected Features ---\")\n",
        "print(importance_df.head(TOP_K_FEATURES))\n",
        "print(\"---------------------------------\")\n",
        "\n",
        "# --- 7. Save the \\\"Pipeline\\\" and Final Data ---\n",
        "\n",
        "# a. Save the list of feature names. This *is* your \\\"pipeline\\\".\n",
        "# In a real app, you would load this list and use it to filter\n",
        "# new, incoming data *after* it has been scaled and hashed.\n",
        "with open(SELECTED_FEATURES_FILE, 'wb') as f:\n",
        "    pickle.dump(final_selected_features, f)\n",
        "print(f\"\\nFeature selection 'pipeline' (list of names) saved to: {SELECTED_FEATURES_FILE}\")\n",
        "\n",
        "# b. Filter your full dataset to *only* these final features\n",
        "X_final_reduced = df_filtered_1[final_selected_features].values\n",
        "\n",
        "print(f\"\\nFinal feature matrix shape: {X_final_reduced.shape}\")\n",
        "\n",
        "# c. Save the new, reduced X matrix and the original string labels\n",
        "np.savez_compressed(\n",
        "    FINAL_REDUCED_DATA_FILE,\n",
        "    X=X_final_reduced,\n",
        "    y_str=y_labels_string  # We save the original string labels for Cell 5\n",
        ")\n",
        "\n",
        "print(f\"Final REDUCED dataset saved to: {FINAL_REDUCED_DATA_FILE}\")\n",
        "print(\"\\n----------------------------------\")\n",
        "print(\"âœ… Cell 4.5 (Feature Reduction) Complete!\")\n",
        "print(\"You are now ready for the final step: Cell 5 (Splitting).\")\n",
        "print(\"----------------------------------\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lS8jFeSDV-wg",
        "outputId": "c3a8324b-046c-43a6-9ecd-1ab712c18e2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from lightgbm) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from lightgbm) (1.16.3)\n",
            "Loading full processed data from: /content/drive/MyDrive/FL_Project/final_processed_data.npz\n",
            "Loading preprocessor info from: /content/drive/MyDrive/FL_Project/preprocessors.pkl\n",
            "Original feature count: 74\n",
            "\n",
            "Running Tech 1: Correlation Filtering (Threshold=0.95)...\n",
            "  Found 16 highly correlated features to drop.\n",
            "  Dropped features: ['packet_length_mean', 'idle_min', 'idle_max', 'flow_iat_max', 'fwd_packet_length_mean', 'average_packet_size', 'active_mean', 'bwd_packet_length_mean', 'fwd_packets_s', 'fwd_iat_min', 'fwd_iat_total', 'packet_length_variance', 'idle_mean', 'flow_iat_min', 'fwd_packet_length_min', 'flow_iat_mean']\n",
            "  Features remaining after correlation filtering: 58\n",
            "\n",
            "Running Tech 2: Tree-Based Selection (Top K=30)...\n",
            "  Training feature selection model...\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.413492 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 10596\n",
            "[LightGBM] [Info] Number of data points in the train set: 1618373, number of used features: 45\n",
            "[LightGBM] [Info] Start training from score -2.393208\n",
            "[LightGBM] [Info] Start training from score -1.298688\n",
            "[LightGBM] [Info] Start training from score -4.961987\n",
            "[LightGBM] [Info] Start training from score -3.254330\n",
            "[LightGBM] [Info] Start training from score -2.415053\n",
            "[LightGBM] [Info] Start training from score -1.376504\n",
            "[LightGBM] [Info] Start training from score -6.102979\n",
            "[LightGBM] [Info] Start training from score -1.401976\n",
            "  Model training complete.\n",
            "\n",
            "--- Top 30 Selected Features ---\n",
            "                       feature  importance\n",
            "23               flow_duration        1521\n",
            "21           packet_length_max        1220\n",
            "17           packet_length_min        1111\n",
            "45               bwd_packets_s        1094\n",
            "6         fwd_segment_size_avg        1029\n",
            "19              flow_packets_s        1015\n",
            "41          bwd_init_win_bytes         996\n",
            "12          fwd_init_win_bytes         981\n",
            "14                flow_bytes_s         964\n",
            "42           packet_length_std         812\n",
            "27                 bwd_iat_min         738\n",
            "20                flow_iat_std         676\n",
            "34       bwd_packet_length_min         641\n",
            "9        fwd_packet_length_max         635\n",
            "3                  fwd_iat_max         633\n",
            "0                  fwd_iat_std         631\n",
            "24                fwd_iat_mean         614\n",
            "29  total_length_of_fwd_packet         591\n",
            "35           fwd_header_length         569\n",
            "10        bwd_segment_size_avg         514\n",
            "36  total_length_of_bwd_packet         453\n",
            "25           bwd_bulk_rate_avg         440\n",
            "28                  active_min         437\n",
            "2                   active_max         410\n",
            "40            total_fwd_packet         356\n",
            "13       bwd_packet_length_max         356\n",
            "32               bwd_iat_total         352\n",
            "1             fwd_seg_size_min         348\n",
            "16                    idle_std         346\n",
            "38       fwd_packet_length_std         333\n",
            "---------------------------------\n",
            "\n",
            "Feature selection 'pipeline' (list of names) saved to: /content/drive/MyDrive/FL_Project/selected_features.pkl\n",
            "\n",
            "Final feature matrix shape: (1618373, 30)\n",
            "Final REDUCED dataset saved to: /content/drive/MyDrive/FL_Project/final_reduced_data.npz\n",
            "\n",
            "----------------------------------\n",
            "âœ… Cell 4.5 (Feature Reduction) Complete!\n",
            "You are now ready for the final step: Cell 5 (Splitting).\n",
            "----------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Section 5: Non-IID Data Splitting for Federated Learning**\n",
        "This crucial section prepares the dataset for a Federated Learning (FL) setup by creating non-IID (non-independently and identically distributed) client datasets. This simulates real-world scenarios where clients may have different data distributions.\n",
        "\n",
        "The steps include:\n",
        "- **Global Test Set Creation**: First, a portion of the total data is set aside as a global test set, stratified by labels to ensure all attack types are represented. This set is for server-side evaluation of the final federated model.\n",
        "- **Client Data Partitioning**: The remaining data is then partitioned into client-specific datasets based on predefined non-IID criteria (e.g., specific attack types assigned to 'hospital' and 'factory' clients).\n",
        "- **Local Train/Test Splits**: Each client's data is further split into local training and testing sets, also stratified by labels.\n",
        "- **Binary Label Conversion**: Attack labels are converted into a binary format (0 for 'Benign', 1 for 'Attack') suitable for binary classification models.\n",
        "- **Saving Client Data**: The processed client training and testing sets, along with the global test set, are saved as compressed `.npz` files, ready to be distributed to FL clients."
      ],
      "metadata": {
        "id": "E6bpCbkm6Ttf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# --- Configuration ---\n",
        "# 1. The final reduced data file from your last step (now an NPZ)\n",
        "FINAL_REDUCED_DATA_FILE = '/content/drive/MyDrive/FL_Project/final_reduced_data.npz'\n",
        "\n",
        "# 2. The final output path for your client .npz files\n",
        "OUTPUT_NPZ_PATH = '/content/drive/MyDrive/FL_Project/client_data/'\n",
        "\n",
        "# 3. How much data to hold out for the final \"Global\" test set\n",
        "GLOBAL_TEST_SET_SIZE = 0.1  # 10%\n",
        "\n",
        "# 4. --- This is your Non-IID Split Logic ---\n",
        "NORMAL_LABEL = 'Benign'\n",
        "CLIENT_1_LABELS = ['Benign', 'Spoofing', 'Web-Based'] # \"Hospital\"\n",
        "CLIENT_2_LABELS = ['Benign', 'Brute Force', 'DOS', 'DDOS', 'Mirai', 'Recon'] # \"Factory\"\n",
        "# -----------------------------------------------\n",
        "\n",
        "print(f\"Loading final reduced dataset from: {FINAL_REDUCED_DATA_FILE}\")\n",
        "# 1. Load your final, reduced data (X and y_str) from the NPZ file\n",
        "data = np.load(FINAL_REDUCED_DATA_FILE, allow_pickle=True)\n",
        "X_final_reduced = data['X']\n",
        "y_labels_string = data['y_str']\n",
        "\n",
        "# Reconstruct a DataFrame for easier handling with existing code that expects it\n",
        "df_final = pd.DataFrame(X_final_reduced)\n",
        "df_final['label'] = y_labels_string # Add the label column back\n",
        "\n",
        "print(\"Load complete.\")\n",
        "\n",
        "# --- 2. NEW: Create the Global Test Set FIRST ---\n",
        "print(f\"\\nCreating Global Test Set (Size: {GLOBAL_TEST_SET_SIZE * 100}%) \")\n",
        "\n",
        "# We split the *entire* dataset into a main (90%) and global test (10%)\n",
        "# We stratify by 'label' to ensure all attack types are in the test set.\n",
        "df_main_train, df_global_test = train_test_split(\n",
        "    df_final,\n",
        "    test_size=GLOBAL_TEST_SET_SIZE,\n",
        "    random_state=42,\n",
        "    stratify=df_final['label']\n",
        ")\n",
        "\n",
        "print(f\"Main data for clients: {len(df_main_train)} samples\")\n",
        "print(f\"Global test set: {len(df_global_test)} samples\")\n",
        "\n",
        "# --- 3. Process and Save the Global Test Set ---\n",
        "# This file is for the SERVER person to evaluate the final model.\n",
        "print(\"\\nProcessing Global Test Set...\")\n",
        "y_str_global_test = df_global_test['label']\n",
        "X_global_test = df_global_test.drop(columns=['label']).values\n",
        "y_binary_global_test = np.where(y_str_global_test == NORMAL_LABEL, 0, 1)\n",
        "\n",
        "global_test_file = os.path.join(OUTPUT_NPZ_PATH, \"global_test_set.npz\")\n",
        "os.makedirs(OUTPUT_NPZ_PATH, exist_ok=True)\n",
        "np.savez_compressed(global_test_file, X=X_global_test, y=y_binary_global_test)\n",
        "print(f\"  Saved Global Test Set to: {global_test_file}\")\n",
        "\n",
        "# --- 4. Apply Non-IID split on the *remaining* data ---\n",
        "print(\"\\nApplying Non-IID split to main training data...\")\n",
        "\n",
        "# We use *only* df_main_train to create the client data\n",
        "df_client_1 = df_main_train[df_main_train['label'].isin(CLIENT_1_LABELS)]\n",
        "df_client_2 = df_main_train[df_main_train['label'].isin(CLIENT_2_LABELS)]\n",
        "\n",
        "print(f\"Client 1 ('hospital') has {len(df_client_1)} total samples.\")\n",
        "print(f\"  Labels: {df_client_1['label'].unique()}\")\n",
        "print(f\"Client 2 ('factory') has {len(df_client_2)} total samples.\")\n",
        "print(f\"  Labels: {df_client_2['label'].unique()}\")\n",
        "\n",
        "# 5. Define the helper function to process and save each client\n",
        "def split_and_save_client(df_client, client_id, drive_path):\n",
        "    print(f\"\\nProcessing {client_id}...\")\n",
        "\n",
        "    y_str = df_client['label']\n",
        "    X = df_client.drop(columns=['label']).values\n",
        "\n",
        "    y_binary = np.where(y_str == NORMAL_LABEL, 0, 1)\n",
        "    print(f\"  Found {np.sum(y_binary == 0)} normal samples and {np.sum(y_binary == 1)} attack samples.\")\n",
        "\n",
        "    # c. Create the 80/20 train/test split *for this client*\n",
        "    #    (This is their *local* train/test set)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y_binary,\n",
        "        test_size=0.2,\n",
        "        random_state=42,\n",
        "        stratify=y_binary\n",
        "    )\n",
        "    print(f\"  Split into {len(y_train)} local train and {len(y_test)} local test samples.\")\n",
        "\n",
        "    # d. Save to the .npz files your client code expects\n",
        "    train_file = os.path.join(drive_path, f\"client_{client_id}_train.npz\")\n",
        "    test_file = os.path.join(drive_path, f\"client_{client_id}_test.npz\")\n",
        "\n",
        "    np.savez_compressed(train_file, X=X_train, y=y_train)\n",
        "    np.savez_compressed(test_file, X=X_test, y=y_test)\n",
        "    print(f\"  Saved data to {train_file} and {test_file}\")\n",
        "\n",
        "    return X.shape[1]\n",
        "\n",
        "# 6. Run the function for both clients\n",
        "num_features_c1 = split_and_save_client(df_client_1, \"hospital\", OUTPUT_NPZ_PATH)\n",
        "num_features_c2 = split_and_save_client(df_client_2, \"factory\", OUTPUT_NPZ_PATH)\n",
        "\n",
        "# --- 7. FINAL, CRITICAL STEP ---\n",
        "print(\"\\n---------------------------------------------------------\")\n",
        "print(\"âœ…âœ…âœ… PREPROCESSING 100% COMPLETE! âœ…âœ…âœ…\")\n",
        "print(\"\\nYour files are ready in Google Drive:\")\n",
        "print(\"  - client_hospital_train.npz (For Client 1 Training)\")\n",
        "print(\"  - client_hospital_test.npz  (For Client 1 *Local* Testing)\")\n",
        "print(\"  - client_factory_train.npz  (For Client 2 Training)\")\n",
        "print(\"  - client_factory_test.npz   (For Client 2 *Local* Testing)\")\n",
        "print(\"  - global_test_set.npz     (For the *Server* to test the final model)\")\n",
        "print(\"\\nIMPORTANT: Go to your 'HERMES_Client/config.py' file and set:\")\n",
        "print(f\"NUM_FEATURES = {num_features_c1}\")\n",
        "print(\"---------------------------------------------------------\")"
      ],
      "metadata": {
        "id": "smQjThmwLNHw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8c959331-64e9-4e8d-c7b5-4ee43ced20fa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading final reduced dataset from: /content/drive/MyDrive/FL_Project/final_reduced_data.npz\n",
            "Load complete.\n",
            "\n",
            "Creating Global Test Set (Size: 10.0%) \n",
            "Main data for clients: 1456535 samples\n",
            "Global test set: 161838 samples\n",
            "\n",
            "Processing Global Test Set...\n",
            "  Saved Global Test Set to: /content/drive/MyDrive/FL_Project/client_data/global_test_set.npz\n",
            "\n",
            "Applying Non-IID split to main training data...\n",
            "Client 1 ('hospital') has 491502 total samples.\n",
            "  Labels: ['Spoofing' 'Benign']\n",
            "Client 2 ('factory') has 1310050 total samples.\n",
            "  Labels: ['DOS' 'DDOS' 'Recon' 'Benign' 'Mirai']\n",
            "\n",
            "Processing hospital...\n",
            "  Found 358468 normal samples and 133034 attack samples.\n",
            "  Split into 393201 local train and 98301 local test samples.\n",
            "  Saved data to /content/drive/MyDrive/FL_Project/client_data/client_hospital_train.npz and /content/drive/MyDrive/FL_Project/client_data/client_hospital_test.npz\n",
            "\n",
            "Processing factory...\n",
            "  Found 358468 normal samples and 951582 attack samples.\n",
            "  Split into 1048040 local train and 262010 local test samples.\n",
            "  Saved data to /content/drive/MyDrive/FL_Project/client_data/client_factory_train.npz and /content/drive/MyDrive/FL_Project/client_data/client_factory_test.npz\n",
            "\n",
            "---------------------------------------------------------\n",
            "âœ…âœ…âœ… PREPROCESSING 100% COMPLETE! âœ…âœ…âœ…\n",
            "\n",
            "Your files are ready in Google Drive:\n",
            "  - client_hospital_train.npz (For Client 1 Training)\n",
            "  - client_hospital_test.npz  (For Client 1 *Local* Testing)\n",
            "  - client_factory_train.npz  (For Client 2 Training)\n",
            "  - client_factory_test.npz   (For Client 2 *Local* Testing)\n",
            "  - global_test_set.npz     (For the *Server* to test the final model)\n",
            "\n",
            "IMPORTANT: Go to your 'HERMES_Client/config.py' file and set:\n",
            "NUM_FEATURES = 30\n",
            "---------------------------------------------------------\n"
          ]
        }
      ]
    }
  ]
}